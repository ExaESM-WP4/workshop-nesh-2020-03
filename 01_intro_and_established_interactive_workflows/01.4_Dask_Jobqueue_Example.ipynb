{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask jobqueue example\n",
    "\n",
    "## What is Dask jobqueue? (https://jobqueue.dask.org/)\n",
    "\n",
    "* deploys Dask workers on typical HPC job queueing systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo estimate with multiple Dask batch job workers\n",
    "We define a Dask jobqueue cluster with Dask workers that each have 4 CPUs and 24 GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask, dask.distributed, os\n",
    "import dask_jobqueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up further Dask configurations in local directory\n",
    "additional_config = dask.config.collect(paths=['.'])\n",
    "dask.config.update(dask.config.config, additional_config, priority='new');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cores': 96,\n",
       " 'memory': '90000M',\n",
       " 'processes': 1,\n",
       " 'local-directory': '/tmp',\n",
       " 'death-timeout': 60,\n",
       " 'extra': ['--host ${SLURMD_NODENAME}.ib.juwels.fzj.de'],\n",
       " 'interface': None,\n",
       " 'shebang': '#!/usr/bin/env bash',\n",
       " 'walltime': '00:15:00',\n",
       " 'log-directory': 'dask_jobqueue_logs',\n",
       " 'name': 'dask-worker',\n",
       " 'queue': None,\n",
       " 'project': None,\n",
       " 'job-cpu': None,\n",
       " 'job-mem': None,\n",
       " 'job-extra': [],\n",
       " 'env-extra': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask.config.get('jobqueue.juwels-jobqueue-config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dask_jobqueue.SLURMCluster(\n",
    "    config_name='juwels-jobqueue-config',\n",
    "    project='esmtst', # specify budget name associated with project\n",
    "    queue='esm', # choose queue by available resources\n",
    "    scheduler_options={\"host\": os.environ['HOSTNAME']}, # globally visible local scheduler network location\n",
    "    cores=16  # divide into 16 processes\n",
    ")\n",
    "\n",
    "client = dask.distributed.Client(cluster)\n",
    "cluster.scale(jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.11.159.193:43091</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.11.159.193:8787/status' target='_blank'>http://10.11.159.193:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>16</li>\n",
       "  <li><b>Memory: </b>90.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.11.159.193:43091' processes=1 threads=16, memory=90.00 GB>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a jobqueue cluster?\n",
    "The above is all we need to specify to run the computation on compute node Dask workers. \n",
    "Let's have a look at what's happening in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           2524579       esm dask-wor    rath1 CF       0:03      1 jwc00n003\n"
     ]
    }
   ],
   "source": [
    "!squeue -u {os.environ[\"USER\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -e dask_jobqueue_logs/dask-worker-%J.err\n",
      "#SBATCH -o dask_jobqueue_logs/dask-worker-%J.out\n",
      "#SBATCH -p esm\n",
      "#SBATCH -A esmtst\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=16\n",
      "#SBATCH --mem=84G\n",
      "#SBATCH -t 00:15:00\n",
      "\n",
      "/p/home/jusers/rath1/juwels/PROJECT_training2005/2020-08_dask_intro/miniconda3/envs/py3_dask/bin/python -m distributed.cli.dask_worker tcp://10.11.159.193:43091 --nthreads 16 --memory-limit 90.00GB --name name --nanny --death-timeout 60 --local-directory /tmp --host ${SLURMD_NODENAME}.ib.juwels.fzj.de\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's scale up the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           2524581       esm dask-wor    rath1 PD       0:00      1 (None)\n",
      "           2524582       esm dask-wor    rath1 PD       0:00      1 (None)\n",
      "           2524583       esm dask-wor    rath1 PD       0:00      1 (None)\n",
      "           2524580       esm dask-wor    rath1  R       0:07      1 jwc00n006\n",
      "           2524579       esm dask-wor    rath1  R       0:20      1 jwc00n003\n"
     ]
    }
   ],
   "source": [
    "!squeue -u {os.environ[\"USER\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.11.159.193:43091</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.11.159.193:8787/status' target='_blank'>http://10.11.159.193:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>16</li>\n",
       "  <li><b>Memory: </b>90.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.11.159.193:43091' processes=1 threads=16, memory=90.00 GB>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here everything is the same as with LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, dask.array\n",
    "\n",
    "def calculate_pi(size_in_bytes, number_of_chunks):\n",
    "    \n",
    "    \"\"\"Calculate pi using a Monte Carlo method.\"\"\"\n",
    "    \n",
    "    array_shape = (int(size_in_bytes / 8 / 2), 2) # tuple\n",
    "    chunk_size = (int(array_shape[0] / number_of_chunks), 2) # tuple\n",
    "    \n",
    "    # 2D random positions array using dask.array\n",
    "    xy = dask.array.random.uniform(\n",
    "        low=0.0, high=1.0, size=array_shape,\n",
    "        # specify chunk size, i.e. task number\n",
    "        chunks=chunk_size )\n",
    "  \n",
    "    xy_inside_circle = (xy ** 2).sum(axis=1) < 1 # boolean\n",
    "\n",
    "    pi = 4 * xy_inside_circle.sum() / xy_inside_circle.size\n",
    "    \n",
    "    # start Dask calculation\n",
    "    pi = pi.compute()\n",
    "\n",
    "    print(f\"\\nfrom {xy.nbytes / 1e9} GB randomly chosen positions\")\n",
    "    print(f\"   pi estimate: {pi}\")\n",
    "    print(f\"   pi error: {abs(pi - numpy.pi)}\\n\")\n",
    "    # display(xy)\n",
    "    \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's calculate again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from 10.0 GB randomly chosen positions\n",
      "   pi estimate: 3.1415992768\n",
      "   pi error: 6.6232102069463394e-06\n",
      "\n",
      "CPU times: user 319 ms, sys: 29.2 ms, total: 349 ms\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%time pi = calculate_pi(size_in_bytes=10_000_000_000, number_of_chunks=100) # 10 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from 100.0 GB randomly chosen positions\n",
      "   pi estimate: 3.14160130624\n",
      "   pi error: 8.652650206997237e-06\n",
      "\n",
      "CPU times: user 902 ms, sys: 71.7 ms, total: 973 ms\n",
      "Wall time: 9.75 s\n"
     ]
    }
   ],
   "source": [
    "%time pi = calculate_pi(size_in_bytes=100_000_000_000, number_of_chunks=250) # 100 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from 1000.0 GB randomly chosen positions\n",
      "   pi estimate: 3.14160087136\n",
      "   pi error: 8.21777020698633e-06\n",
      "\n",
      "CPU times: user 8.35 s, sys: 631 ms, total: 8.98 s\n",
      "Wall time: 57.8 s\n"
     ]
    }
   ],
   "source": [
    "%time pi = calculate_pi(size_in_bytes=1_000_000_000_000, number_of_chunks=1000) # 1 TB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can scale up the cluster whenever needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           2524584       esm dask-wor    rath1 PD       0:00      1 (Resources)\n",
      "           2524581       esm dask-wor    rath1  R       0:33      1 jwc00n009\n",
      "           2524582       esm dask-wor    rath1  R       0:33      1 jwc00n012\n",
      "           2524583       esm dask-wor    rath1  R       0:33      1 jwc00n015\n",
      "           2524580       esm dask-wor    rath1  R       1:20      1 jwc00n006\n",
      "           2524579       esm dask-wor    rath1  R       1:33      1 jwc00n003\n"
     ]
    }
   ],
   "source": [
    "!squeue -u {os.environ[\"USER\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.11.159.193:43091</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.11.159.193:8787/status' target='_blank'>http://10.11.159.193:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>5</li>\n",
       "  <li><b>Cores: </b>80</li>\n",
       "  <li><b>Memory: </b>450.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.11.159.193:43091' processes=5 threads=80, memory=450.00 GB>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's calculate again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from 1000.0 GB randomly chosen positions\n",
      "   pi estimate: 3.141598500224\n",
      "   pi error: 5.846634206996271e-06\n",
      "\n",
      "CPU times: user 6.32 s, sys: 591 ms, total: 6.91 s\n",
      "Wall time: 42.3 s\n"
     ]
    }
   ],
   "source": [
    "%time pi = calculate_pi(size_in_bytes=1_000_000_000_000, number_of_chunks=1000) # 1 TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time pi = calculate_pi(size_in_bytes=10_000_000_000_000, number_of_chunks=10000) # 10 TB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note, we could also adaptively scale the jobqueue cluster!\n",
    "\n",
    "Dask jobqueue is able to scale total worker number based on problem size. You can also specify a target duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<distributed.deploy.adaptive.Adaptive at 0x2b062ba7f9d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.adapt(\n",
    "    minimum=1, maximum=5,  # between 1 and 5 jobs\n",
    "    target_duration=\"60s\"  # try to be done after 60 seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           2524579       esm dask-wor    rath1  R       2:48      1 jwc00n003\n"
     ]
    }
   ],
   "source": [
    "!squeue -u {os.environ[\"USER\"]}  # will scale down to one job if idle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from 10.0 GB randomly chosen positions\n",
      "   pi estimate: 3.1415473728\n",
      "   pi error: 4.528078979326722e-05\n",
      "\n",
      "CPU times: user 417 ms, sys: 29.6 ms, total: 447 ms\n",
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "# short calculations don't lead to scaling up\n",
    "%time pi = calculate_pi(size_in_bytes=10_000_000_000, number_of_chunks=100) # 10 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from 1000.0 GB randomly chosen positions\n",
      "   pi estimate: 3.141599735488\n",
      "   pi error: 7.081898206973136e-06\n",
      "\n",
      "CPU times: user 7.21 s, sys: 582 ms, total: 7.79 s\n",
      "Wall time: 46.5 s\n"
     ]
    }
   ],
   "source": [
    "# longer calculations will try to increase cluster size:\n",
    "%time pi = calculate_pi(size_in_bytes=1_000_000_000_000, number_of_chunks=1000) # 1 TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           2524603       esm dask-wor    rath1  R       0:43      1 jwc00n006\n",
      "           2524604       esm dask-wor    rath1  R       0:43      1 jwc00n009\n",
      "           2524605       esm dask-wor    rath1  R       0:43      1 jwc00n012\n",
      "           2524606       esm dask-wor    rath1  R       0:43      1 jwc00n015\n",
      "           2524579       esm dask-wor    rath1  R       4:10      1 jwc00n003\n"
     ]
    }
   ],
   "source": [
    "!squeue -u {os.environ[\"USER\"]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_dask]",
   "language": "python",
   "name": "conda-env-py3_dask-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
